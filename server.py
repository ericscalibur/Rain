#!/usr/bin/env python3
"""
Rain â›ˆï¸ - Web Server (Phase 4)

FastAPI backend serving Rain's multi-agent capabilities via HTTP.
Runs entirely locally at http://localhost:7734
Zero cloud. Zero tracking. Fully sovereign.

Usage:
    .venv/bin/uvicorn server:app --host 127.0.0.1 --port 7734 --reload
"""

import asyncio
import json
import sys
import threading
import time
import uuid
from contextlib import asynccontextmanager
from datetime import datetime
from pathlib import Path
from typing import AsyncGenerator, Optional

from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

# Add Rain's directory to path so we can import rain.py
sys.path.insert(0, str(Path(__file__).parent))

from rain import (
    MultiAgentOrchestrator,
    RainMemory,
    AgentType,
    CodeSandbox,
)

# â”€â”€ App â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€ Global orchestrator (initialized once on startup) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_orchestrator: Optional[MultiAgentOrchestrator] = None
_memory: Optional[RainMemory] = None
_custom_agents: list = []  # [{id, name, prompt}]


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize Rain on startup, clean up on shutdown."""
    global _orchestrator, _memory
    _memory = RainMemory()
    _memory.start_session(model="llama3.1")
    _orchestrator = MultiAgentOrchestrator(
        default_model="llama3.1",
        memory=_memory,
        sandbox_enabled=False,  # user can toggle per request
    )
    yield
    if _memory:
        summary = _memory.generate_summary()
        _memory.end_session()
        if summary:
            _memory.update_summary(summary)


app = FastAPI(
    title="Rain",
    description="Sovereign AI Ecosystem â€” local web interface",
    version="0.4.0",
    docs_url=None,   # disable swagger UI (keep it minimal)
    redoc_url=None,
    lifespan=lifespan,
)

app.mount("/static", StaticFiles(directory="static"), name="static")


# â”€â”€ Request / Response models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ChatRequest(BaseModel):
    message: str
    sandbox: bool = False
    sandbox_timeout: int = 10
    verbose: bool = False
    history: list = []  # recent messages [{role, content}] from the browser
    web_search: bool = False


class CustomAgentRequest(BaseModel):
    name: str
    prompt: str


def _duckduckgo_search(query: str, max_results: int = 5) -> list:
    """
    Search DuckDuckGo using their free HTML endpoint. No API key required.
    Returns a list of {title, snippet, url} dicts.
    """
    import urllib.request
    import urllib.parse
    import re

    try:
        params = urllib.parse.urlencode({"q": query, "kl": "us-en"})
        url = f"https://html.duckduckgo.com/html/?{params}"
        req = urllib.request.Request(
            url,
            headers={
                "User-Agent": "Mozilla/5.0 (compatible; Rain/1.0)",
                "Accept-Language": "en-US,en;q=0.9",
            },
        )
        with urllib.request.urlopen(req, timeout=8) as resp:
            html = resp.read().decode("utf-8", errors="replace")

        results = []
        # Extract result blocks
        blocks = re.findall(
            r'<a[^>]+class="result__a"[^>]*href="([^"]+)"[^>]*>(.*?)</a>.*?'
            r'<a[^>]+class="result__snippet"[^>]*>(.*?)</a>',
            html,
            re.DOTALL,
        )
        for url_raw, title_raw, snippet_raw in blocks[:max_results]:
            title = re.sub(r"<[^>]+>", "", title_raw).strip()
            snippet = re.sub(r"<[^>]+>", "", snippet_raw).strip()
            # DuckDuckGo wraps URLs â€” extract the real one
            url_match = re.search(r"uddg=([^&]+)", url_raw)
            real_url = urllib.parse.unquote(url_match.group(1)) if url_match else url_raw
            if title and snippet:
                results.append({"title": title, "snippet": snippet, "url": real_url})

        return results
    except Exception as e:
        print(f"[web search] failed: {e}")
        return []


class SessionSummary(BaseModel):
    id: str
    started_at: str
    ended_at: Optional[str]
    summary: Optional[str]
    model: Optional[str]
    message_count: int


# â”€â”€ Routes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@app.get("/", response_class=HTMLResponse)
async def root():
    """Serve the single-page UI."""
    html_path = Path(__file__).parent / "static" / "index.html"
    if not html_path.exists():
        raise HTTPException(status_code=404, detail="UI not found â€” static/index.html missing")
    return HTMLResponse(content=html_path.read_text(encoding="utf-8"))


@app.get("/api/health")
async def health():
    """Health check â€” confirms server and Ollama are reachable."""
    import subprocess
    try:
        result = subprocess.run(
            ["ollama", "list"], capture_output=True, text=True, timeout=5
        )
        ollama_ok = result.returncode == 0
    except Exception:
        ollama_ok = False

    return {
        "status": "ok",
        "ollama": ollama_ok,
        "model": _orchestrator.default_model if _orchestrator else None,
        "timestamp": datetime.now().isoformat(),
    }


@app.get("/api/models")
async def list_models():
    """Return installed Ollama models."""
    import subprocess
    try:
        result = subprocess.run(
            ["ollama", "list"], capture_output=True, text=True, timeout=5
        )
        models = []
        for line in result.stdout.strip().split("\n")[1:]:
            if line.strip():
                parts = line.split()
                models.append({
                    "name": parts[0],
                    "size": parts[2] if len(parts) > 2 else "",
                })
        return {"models": models}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/agents")
async def get_agents():
    """Return the current agent roster."""
    if not _orchestrator:
        raise HTTPException(status_code=503, detail="Orchestrator not initialized")

    roster = []
    for agent_type in [AgentType.DEV, AgentType.LOGIC, AgentType.DOMAIN,
                       AgentType.REFLECTION, AgentType.SYNTHESIZER]:
        agent = _orchestrator.agents[agent_type]
        specialized = not agent.model_name.startswith(
            _orchestrator.default_model.split(":")[0]
        )
        roster.append({
            "type": agent.agent_type.value,
            "description": agent.description,
            "model": agent.model_name,
            "specialized": specialized,
        })

    missing = []
    from rain import AGENT_PREFERRED_MODELS
    for agent_type, preferred in AGENT_PREFERRED_MODELS.items():
        if agent_type in (AgentType.REFLECTION, AgentType.SYNTHESIZER, AgentType.GENERAL):
            continue
        best = preferred[0]
        if not any(m.startswith(best.split(":")[0])
                   for m in _orchestrator._installed_models):
            missing.append(best)

    return {
        "roster": roster,
        "suggestions": list(dict.fromkeys(missing)),
        "custom_agents": _custom_agents,
    }


@app.post("/api/agents/custom")
async def add_custom_agent(req: CustomAgentRequest):
    """Register a new custom agent with a user-defined system prompt."""
    global _custom_agents, _orchestrator
    if not req.name.strip() or not req.prompt.strip():
        raise HTTPException(status_code=400, detail="Name and prompt are required")

    agent_id = str(uuid.uuid4())[:8]
    entry = {"id": agent_id, "name": req.name.strip(), "prompt": req.prompt.strip()}
    _custom_agents.append(entry)

    # Register with orchestrator so it can be routed to
    if _orchestrator:
        from rain import Agent, AgentType
        agent = Agent(
            agent_type=AgentType.GENERAL,
            model_name=_orchestrator.default_model,
            system_prompt=req.prompt.strip(),
            description=req.name.strip(),
        )
        _orchestrator.custom_agents[agent_id] = agent

    return {"status": "ok", "id": agent_id}


@app.delete("/api/agents/custom/{agent_id}")
async def remove_custom_agent(agent_id: str):
    """Remove a custom agent by ID."""
    global _custom_agents, _orchestrator
    _custom_agents = [a for a in _custom_agents if a["id"] != agent_id]
    if _orchestrator and hasattr(_orchestrator, "custom_agents"):
        _orchestrator.custom_agents.pop(agent_id, None)
    return {"status": "ok"}


@app.get("/api/sessions")
async def get_sessions():
    """Return recent session history."""
    if not _memory:
        raise HTTPException(status_code=503, detail="Memory not initialized")
    sessions = _memory.get_recent_sessions(limit=20)
    return {"sessions": sessions}


@app.get("/api/sessions/{session_id}/messages")
async def get_session_messages(session_id: str):
    """Return all messages for a given session."""
    if not _memory:
        raise HTTPException(status_code=503, detail="Memory not initialized")
    import sqlite3
    with sqlite3.connect(_memory.db_path) as conn:
        conn.row_factory = sqlite3.Row
        rows = conn.execute(
            """SELECT role, content, timestamp, is_code, confidence
               FROM messages WHERE session_id = ?
               ORDER BY timestamp ASC""",
            (session_id,),
        ).fetchall()
    return {"messages": [dict(r) for r in rows]}


@app.post("/api/new-session")
async def new_session():
    """End the current session and start a fresh one."""
    global _memory, _orchestrator
    if not _memory:
        raise HTTPException(status_code=503, detail="Memory not initialized")

    import threading

    # Hold a reference to the ending session, end it synchronously so it
    # appears in the sidebar immediately (ended_at IS NOT NULL filter)
    ending_memory = _memory
    ending_memory.end_session()

    # Create a brand-new RainMemory â€” generates a fresh UUID
    _memory = RainMemory()
    _memory.start_session(model=_orchestrator.default_model if _orchestrator else "llama3.1")
    if _orchestrator:
        _orchestrator.memory = _memory

    # Summarize the ended session in the background
    def _summarize_in_background():
        summary = ending_memory.generate_summary()
        if summary:
            ending_memory.update_summary(summary)
    threading.Thread(target=_summarize_in_background, daemon=True).start()

    return {"status": "ok", "session_id": _memory.session_id}


@app.post("/api/forget")
async def forget():
    """Wipe all memory."""
    if not _memory:
        raise HTTPException(status_code=503, detail="Memory not initialized")
    _memory.forget_all()
    return {"status": "ok", "message": "All memory wiped."}


@app.post("/api/chat")
async def chat(req: ChatRequest):
    """
    Main chat endpoint. Returns a Server-Sent Events stream.

    Each event is a JSON object with a `type` field:
      { type: "routing",   agent: "Domain Expert ..." }
      { type: "progress",  message: "Primary response ready..." }
      { type: "progress",  message: "Reflection complete (rating: GOOD)" }
      { type: "progress",  message: "Synthesizing improvements..." }
      { type: "sandbox",   message: "Testing block 1/2...", status: "running" }
      { type: "sandbox",   message: "âœ… verified", status: "ok" }
      { type: "done",      content: "...", confidence: 0.75, duration: 12.3,
                           sandbox_verified: true, agent: "dev" }
      { type: "error",     message: "..." }
    """
    if not _orchestrator:
        raise HTTPException(status_code=503, detail="Orchestrator not initialized")

    return StreamingResponse(
        _stream_chat(req),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
        },
    )


async def _stream_chat(req: ChatRequest) -> AsyncGenerator[str, None]:
    """
    Run the multi-agent pipeline in a thread and stream SSE events back
    as each stage completes. Uses a queue to bridge the sync orchestrator
    and the async SSE stream.
    """
    queue: asyncio.Queue = asyncio.Queue()
    loop = asyncio.get_event_loop()

    def emit(event: dict):
        """Thread-safe event emitter â€” puts onto the asyncio queue."""
        loop.call_soon_threadsafe(queue.put_nowait, event)

    def run_pipeline():
        """Runs in a background thread â€” calls the synchronous orchestrator."""
        try:
            # Temporarily patch the orchestrator to emit SSE events at each stage
            original_route = _orchestrator.router.route

            def patched_route(query):
                result = original_route(query)
                emit({
                    "type": "routing",
                    "agent": _orchestrator.router.explain(result),
                    "agent_type": result.value,
                })
                return result

            _orchestrator.router.route = patched_route

            # Configure sandbox for this request
            _orchestrator.sandbox_enabled = req.sandbox
            if req.sandbox:
                _orchestrator.sandbox = CodeSandbox(timeout=req.sandbox_timeout)
            else:
                _orchestrator.sandbox = None

            # Monkey-patch _query_agent to emit progress events
            original_query_agent = _orchestrator._query_agent

            def patched_query_agent(agent, prompt, label=None):
                emit({"type": "progress", "message": f"ğŸ’­ {label or agent.description + ' thinking...'}"})
                result = original_query_agent(agent, prompt, label=label)
                return result

            _orchestrator._query_agent = patched_query_agent

            # Monkey-patch reflection rating emit
            original_parse_rating = _orchestrator._parse_reflection_rating

            def patched_parse_rating(critique):
                rating = original_parse_rating(critique)
                emit({"type": "progress", "message": f"ğŸ” Reflection complete (rating: {rating})"})
                return rating

            _orchestrator._parse_reflection_rating = patched_parse_rating

            # â”€â”€ Web search (if enabled) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            search_results_count = 0
            search_augmented_message = None
            if req.web_search:
                emit({"type": "progress", "message": "ğŸŒ Searching the web..."})
                search_results = _duckduckgo_search(req.message, max_results=5)
                search_results_count = len(search_results)
                if search_results:
                    snippets = "\n\n".join(
                        f"[{r['title']}]\n{r['snippet']}\nSource: {r['url']}"
                        for r in search_results
                    )
                    emit({"type": "progress", "message": f"ğŸŒ {search_results_count} results retrieved"})
                    search_augmented_message = (
                        f"[Web search results for: {req.message}]\n\n"
                        f"{snippets}\n\n"
                        f"---\n"
                        f"Using the above search results as context, answer this question accurately. "
                        f"Cite sources where relevant. If the search results don't contain enough "
                        f"information, say so.\n\n"
                        f"Question: {req.message}"
                    )
                else:
                    emit({"type": "progress", "message": "ğŸŒ No results found â€” using local knowledge"})

            # Only inject the single preceding exchange for short follow-ups
            # (e.g. "yes", "elaborate", "continue", "tell me more").
            # Longer messages have enough context on their own.
            query = search_augmented_message if search_augmented_message else req.message
            is_short_followup = len(req.message.strip()) < 60
            if is_short_followup and req.history and len(req.history) >= 2:
                import re as _re
                # Find the last user message and last Rain response
                last_user = next(
                    (m for m in reversed(req.history) if m.get("role") == "user"), None
                )
                last_rain = next(
                    (m for m in reversed(req.history) if m.get("role") == "assistant"), None
                )
                if last_user and last_rain:
                    def _strip_code(text):
                        return _re.sub(r'```[\s\S]*?```', '[code]', text).strip()[:250]
                    prev_q = _strip_code(last_user.get("content", ""))
                    prev_a = _strip_code(last_rain.get("content", ""))
                    if prev_q and prev_a:
                        query = (
                            f"[Previous exchange:\n"
                            f"User: {prev_q}\n"
                            f"Rain: {prev_a}\n"
                            f"]\n\n"
                            f"User follow-up: {req.message}"
                        )

            # Run the full pipeline
            result = _orchestrator.recursive_reflect(query, verbose=req.verbose)

            # Restore originals
            _orchestrator.router.route = original_route
            _orchestrator._query_agent = original_query_agent
            _orchestrator._parse_reflection_rating = original_parse_rating

            if result:
                sandbox_summary = None
                if result.sandbox_results:
                    def _is_network(r):
                        err = (r.stderr + (r.error_message or "")).lower()
                        return not r.success and any(x in err for x in [
                            "urlerror", "connectionrefused", "nodename nor servname",
                            "name or service not known", "network is unreachable",
                            "connection timed out", "urlopen error", "ssl",
                        ])
                    def _is_long_running(r):
                        err = (r.error_message or "").lower()
                        return not r.success and "long-running" in err

                    skipped = [r for r in result.sandbox_results if _is_network(r) or _is_long_running(r)]
                    testable = [r for r in result.sandbox_results if r not in skipped]
                    verified = sum(1 for r in testable if r.success)
                    total = len(testable)
                    sandbox_summary = {
                        "verified": verified,
                        "total": total,
                        "all_ok": verified == total if total > 0 else True,
                        "network": any(_is_network(r) for r in skipped),
                        "long_running": any(_is_long_running(r) for r in skipped),
                    }

                emit({
                    "type": "done",
                    "content": result.content,
                    "confidence": round(result.confidence, 2),
                    "duration": round(result.duration_seconds, 1),
                    "iterations": result.iteration,
                    "improvements": result.improvements,
                    "sandbox": sandbox_summary,
                    "search_results": search_results_count if req.web_search else 0,
                })
            else:
                emit({"type": "error", "message": "No response from model."})

        except Exception as e:
            emit({"type": "error", "message": str(e)})
        finally:
            emit({"type": "_done_sentinel"})

    # Start pipeline in background thread
    thread = threading.Thread(target=run_pipeline, daemon=True)
    thread.start()

    # Stream events from queue until sentinel received
    while True:
        event = await queue.get()
        if event.get("type") == "_done_sentinel":
            break
        yield f"data: {json.dumps(event)}\n\n"
        await asyncio.sleep(0)  # yield control to event loop


# â”€â”€ Entry point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "server:app",
        host="127.0.0.1",
        port=7734,
        reload=True,
        log_level="warning",
    )
